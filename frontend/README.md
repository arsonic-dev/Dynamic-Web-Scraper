# ğŸš€ Dynamic Web Scraper

A full-stack scraping platform built with **React + TypeScript (Vite)** and **FastAPI (Python)**.

Submit scraping jobs, track progress, validate structured data, and download results â€” all from a clean dashboard interface.

---

## âœ¨ Features

- âœ… Submit scraping jobs from a modern dashboard
- âœ… Real-time job status tracking
- âœ… Paginated job history view
- âœ… Structured API contract (Frontend â†” Backend aligned)
- âœ… Data quality scoring system
- âœ… Download endpoint for CSV results (mock)
- âœ… Fully documented REST API (Swagger)
- âœ… Clean responsive dark UI
- âœ… Modular service-layer architecture

---

## âš™ï¸ How It Works

### 1ï¸âƒ£ Job Submission
User submits:
- Target URL
- Data type
- Number of pages
- Headless mode

Frontend sends:


---

### 2ï¸âƒ£ Backend Processing
Backend:
- Generates unique job ID
- Simulates scraping process
- Calculates mock data quality score
- Stores job in memory

---

### 3ï¸âƒ£ Status Tracking

Frontend polls:


Returns:
- Progress percentage
- Records extracted
- Completion status

---

### 4ï¸âƒ£ View Results

Displays all previously submitted jobs with pagination support.

---

## ğŸ— Tech Stack

### Frontend
- React
- TypeScript
- Vite
- Axios
- Tailwind CSS
- Framer Motion

### Backend
- FastAPI
- Uvicorn
- Pydantic
- Python 3.11+

---

## ğŸ”Œ Integrations

- Swagger API documentation (`/docs`)
- RESTful contract-driven API
- Axios service layer for structured requests
- CORS-enabled frontend-backend communication
- Modular backend route architecture

---

## ğŸ“‚ Project Structure

Dynamic-Web-Scraper/
â”‚
â”œâ”€â”€ frontend/
â”‚ â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ components/
â”‚ â”œâ”€â”€ pages/
â”‚ â””â”€â”€ services/api.ts
â”‚
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ main.py
â”‚ â””â”€â”€ requirements.txt
â”‚
â””â”€â”€ README.md


---

## ğŸš€ Getting Started

### Backend

```bash
cd backend
python -m venv venv
venv\Scripts\activate   # Windows
pip install -r requirements.txt
python -m uvicorn main:app --reload --port 8000

API Docs:

http://localhost:8000/docs

Frontend
cd frontend
npm install
npm run dev

Frontend:

http://localhost:3000



ğŸ§  Current Version

This version uses a mock scraping engine for testing the full-stack workflow.

ğŸ›£ Roadmap

 Replace mock engine with Selenium execution

 Add CSV file generation

 Add database persistence (PostgreSQL)

 Add background job queue

 Add authentication

 Dockerize application

 Deploy production version



ğŸ“Œ Future Vision

This project is designed to evolve into:

A structured scraping SaaS platform

A scalable data validation engine

A distributed scraping microservice system


ğŸ‘¨â€ğŸ’» Author

Ankit Kumar
arsonnick349@gmail.com
arsonic-dev
Full-stack system built for structured scraping automation and API-driven architecture learning.